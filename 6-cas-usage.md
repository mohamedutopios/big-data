parfait â€” voici un module **trÃ¨s bien dÃ©coupÃ©**, avec **explications, choix dâ€™architecture, schÃ©mas ASCII, donnÃ©es dâ€™exemple, mÃ©triques, pseudo-code et snippets**. Tu peux lâ€™intÃ©grer tel quel dans ton support et tâ€™en servir pour une dÃ©mo live.

---

# ğŸ§ª Cas dâ€™usage concrets & dÃ©monstrations

## 1) DÃ©tection de fraude bancaire en temps rÃ©el

### ğŸ¯ Objectif mÃ©tier

RÃ©duire les pertes liÃ©es Ã  la fraude **sans bloquer** les clients lÃ©gitimes (Ã©quilibre entre **rappel** et **faux positifs**), dÃ©cision < **50 ms**.

### ğŸ”— Sources & schÃ©ma minimal dâ€™Ã©vÃ©nement (Kafka `transactions`)

* **Key** = `transaction_id`
* **Value (JSON/Avro)** :

```json
{
  "transaction_id":"tx_987654",
  "timestamp":"2025-08-16T12:00:05Z",
  "account_id":"acc_123",
  "card_hash":"4a1f...ed",
  "amount": 249.90,
  "currency":"EUR",
  "merchant_id":"m_445",
  "merchant_category":"electronics",
  "channel":"ecom",
  "country":"FR",
  "device_id":"dvc_771",
  "ip":"83.12.45.21",
  "lat":48.85, "lon":2.35
}
```

### ğŸ§± Architecture logique

```
[Apps/Terminals] â†’ Kafka (transactions) â†’ Stream compute (Flink/Spark)
                                  â”‚                  â”‚
                                  â”‚                  â”œâ”€> Online features (Redis/Feature Store)
                                  â”‚                  â”œâ”€> Model Serving (REST/gRPC, <20ms)
                                  â”‚                  â””â”€> Decision Engine (rules + score)
                                  â””â”€> Object Storage (S3/ADLS) â†’ Training (batch) â†’ Registry (MLflow)
```

### ğŸ› ï¸ Features critiques (fenÃªtres & graphes)

* **VÃ©locitÃ© par carte / device / IP** (1min/5min/1h): `count_tx_1m`, `sum_amount_5m`, `uniq_merchant_1h`
* **DÃ©rive gÃ©ographique**: distance (Haversine) vs derniÃ¨re transaction, `km_per_min`
* **Profil temporel**: z-score montant vs moyenne 7j (`(amount - mean_7d)/std_7d`)
* **Risque marchand**: frÃ©quence fraude historique par `merchant_id` ou `MCC`
* **Graph features** (si dispo): communs `device_id` â†” `account_id` â†” `ip` (communautÃ©s anormales)

**Exemple calcul pas-Ã -pas (z-score)**
Moyenne 7j = 60 â‚¬, Ã©cart-type 7j = 40 â‚¬. Nouvelle transaction = 249,90 â‚¬
`z = (249,90 - 60) / 40 = 4,75` â†’ anomalie forte.

### ğŸ¤– ModÃ¨les & dÃ©cision

* **Temps rÃ©el**: Gradient Boosted Trees / XGBoost / LightGBM (robuste, rapide) + **rÃ¨gles explicites** (cap sur montant, pays blacklistÃ©s, 3D Secure manquÃ©â€¦).
* **Anomalie**: Isolation Forest / Autoencoder (dÃ©tection zero-day).
* **DÃ©cision**: `ALLOW` si score<0.5 ; `CHALLENGE` (3DS/SCA) si 0.5â€“0.8 ; `BLOCK` >0.8.
  **Reason codes** loggÃ©s (explicabilitÃ©: SHAP top-features).

### ğŸ“ˆ KPIs & surveillance

* **Capture fraude** (recall) â†‘ ; **false positive rate** â†“ ; **latence P95** < 50 ms ; **â‚¬ fraude Ã©vitÃ©e** ; **taux frictions**.
* **Drift**: distribution features vs training ; recalibrage mensuel.

### ğŸ”§ Snippet (Structured Streaming PySpark â€“ fenÃªtres & score factice)

```python
# lecture Kafka â†’ JSON
df = (spark.readStream.format("kafka")
      .option("kafka.bootstrap.servers","kafka:9092")
      .option("subscribe","transactions")
      .load())

from pyspark.sql.functions import from_json, col, window, avg, stddev, count
schema = "transaction_id string, timestamp timestamp, account_id string, amount double, merchant_id string, device_id string, ip string, country string"
json = df.selectExpr("CAST(value AS STRING) v").select(from_json(col("v"), schema).alias("j")).select("j.*")

# fenÃªtres 1 minute par compte
win = (json
  .withWatermark("timestamp","2 minutes")
  .groupBy("account_id", window("timestamp","1 minute"))
  .agg(count("*").alias("cnt_1m"), avg("amount").alias("avg_1m"), stddev("amount").alias("std_1m")))

# jointure simple (ex.: seuils mÃ©tier en mÃ©moire)
scored = win.selectExpr("account_id","cnt_1m","avg_1m","coalesce(std_1m,0.01) as std_1m")\
            .withColumn("score", (col("cnt_1m")/5.0) + (col("avg_1m")/200.0))

(scored.writeStream
 .outputMode("update")
 .format("memory")
 .queryName("fraud_scores")
 .start())
```

---

## 2) Analyse de sentiments sur les rÃ©seaux sociaux

### ğŸ¯ Objectif

Mesurer en **quasi temps rÃ©el** la perception dâ€™une marque/campagne, dÃ©tecter les **pics nÃ©gatifs** (crise), informer le **service client**.

### ğŸ”— DonnÃ©es & pipeline

* Connecteurs API (X/Twitter, Reddit, avis, forums, TikTok via fournisseurs), **langID**, horodatage, mÃ©triques dâ€™engagement, texte.
* **Kafka** topics : `social_raw`, `social_clean`, `social_sentiment`.

**Ã‰vÃ©nement (simplifiÃ©)**

```json
{"post_id":"p_889","ts":"2025-08-16T10:04:00Z","source":"twitter","lang":"fr",
 "text":"Livraison en retard, trÃ¨s dÃ©Ã§u !","engagement":{"likes":2,"replies":1}}
```

### ğŸ§¼ PrÃ©-traitement

* DÃ©duplication, dÃ©tection langue, nettoyage (URLs, emojis â†’ tokens), normalisation (lowercase), stopwords spÃ©cifiques, correction lÃ©gÃ¨re.

### ğŸ¤– ModÃ©lisation

* **ModÃ¨le**: Transformer multilingue fine-tuned (3 classes pos/neu/neg) + **aspects** (produit, prix, SAV) via **topic/aspect extraction** (KeyBERT / BERTopic).
* **Sorties**: `label`, `confidence`, `aspects[]`.
* **AgrÃ©gation**: **indice de sentiment pondÃ©rÃ©** par engagement, par **jour/source/aspect**.

### ğŸ“Š Exemple dâ€™agrÃ©gation & alerte

* Sur 1h: 150 posts, 60 neg, 30 neu, 60 pos â†’ score = `(pos - neg)/(total)` = 0.0 â†’ neutre ;
* Pic nÃ©gatif si score < âˆ’0.25 **et** `neg > mean_7d_neg + 2Ïƒ` â†’ Slack/PagerDuty.

### ğŸ–¥ï¸ Dashboard

* Courbe sentiment (P5/P50/P95), **wordcloud nÃ©gatif**, top aspects nÃ©gatifs, temps de rÃ©ponse SAV, comparatif concurrents.

### âš ï¸ PiÃ¨ges & remÃ¨des

* **Sarcasme**: utiliser emojis/punctuations comme features ;
* **Bruits/Spams**: rÃ¨gles + modÃ¨les de spam ;
* **Biais langue**: Ã©valuer par langue ; dataset annotÃ© Ã©quilibrÃ©.

---

## 3) ChaÃ®ne logistique prÃ©dictive (Demand & ETA + rÃ©assort)

### ğŸ¯ Objectif

RÃ©duire **ruptures** et **stocks morts**, amÃ©liorer **OTIF** (On-Time-In-Full) et **taux de service**.

### ğŸ”— DonnÃ©es

* Commandes (SKU, quantitÃ©, prix, canal), stocks, historiques ventes, **lead time** fournisseurs, promotions, mÃ©tÃ©o/jours fÃ©riÃ©s, capteurs transport (tempÃ©rature, position), coÃ»ts.

### ğŸ§® PrÃ©visions & rÃ©assort

* **Demand forecasting** par SKU/site: modÃ¨les hiÃ©rarchiques (Prophet, ETS), gradient boosting (XGBoost) avec promos/prix/concurrence.
* **ROP (Reorder Point)** :

  * Moyenne demande/jour `Î¼_d`, Ã©cart-type `Ïƒ_d` ; lead time moyen `Î¼_L`, Ã©cart-type `Ïƒ_L`.
  * Demande moyenne pendant lead time `Î¼ = Î¼_d * Î¼_L`.
  * Ã‰cart-type pendant lead time `Ïƒ = sqrt( Î¼_L * Ïƒ_dÂ² + Î¼_dÂ² * Ïƒ_LÂ² )`.
  * **Safety stock** = `z * Ïƒ` (ex. z=1,65 pour 95%).
  * **ROP** = `Î¼ + safety stock`.

**Exemple chiffrÃ© (clair et reproductible)**
`Î¼_d=100`, `Ïƒ_d=20`, `Î¼_L=5`, `Ïƒ_L=1`
`Î¼ = 100*5 = 500`
`Ïƒ = sqrt(5*400 + 100^2*1) = sqrt(2000 + 10000) = sqrt(12000) â‰ˆ 109,54`
Safety stock = `1,65 * 109,54 â‰ˆ 180,75` â†’ **181**
**ROP â‰ˆ 500 + 181 = 681 unitÃ©s**

### ğŸšš ETA prÃ©dictive & risques transport

* Features: route, transporteur, mÃ©tÃ©o, trafic, jour/heure, historique retard, saturation quai.
* ModÃ¨les: GBDT / time-to-event (survival).
* Actions: **rÃ©-allocation** stock, re-routing, alerte magasin.

### ğŸ“ˆ KPIs

MAPE prÃ©vision â†“, taux de rupture â†“, stock moyen â†“, OTIF â†‘, **inventory turns** â†‘, **â‚¬ working capital** â†“.

---

# âš™ï¸ DÃ©mo conceptuelle â€“ Pipeline simple (Kafka â†’ Data Lake â†’ Spark â†’ BI)

## ğŸ§© SchÃ©ma gÃ©nÃ©ral

```
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Producers    â”‚  (apps web, capteurs, APIs)
           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚  (events JSON/Avro)
             Kafka Topics
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ orders   â”‚ inventory â”‚ social    â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚           â”‚           â”‚
          â–¼           â–¼           â–¼
   Spark Structured Streaming (jobs)
          â”‚           â”‚           â”‚
          â””â”€â”€â”€â”€â†’ Data Lake (raw â†’ silver â†’ gold) [Parquet/Delta]
                           â”‚
                           â–¼
              BI / Analytics (Tableau / Power BI / Databricks SQL)
```

## 1) Ingestion (Kafka)

### a) Topics & schÃ©mas

* `orders` (clÃ©: `order_id`) â€“ commandes
* `inventory` â€“ mises Ã  jour stock
* `social_raw` â€“ posts rÃ©seaux sociaux

**Exemple `orders` (JSON)**

```json
{"order_id":"o_1001","ts":"2025-08-16T12:10:00Z","customer_id":"c_77",
 "items":[{"sku":"A123","qty":2,"price":19.9},{"sku":"B200","qty":1,"price":49}],
 "channel":"web","store_id":null,"city":"Lille","country":"FR"}
```

### b) Bonnes pratiques

* **Avro/Protobuf + Schema Registry** (compatibilitÃ© schÃ©ma)
* ClÃ©s bien choisies (partitionnement par `customer_id`/`store_id` si besoin dâ€™ordering)

## 2) Stockage (Data Lake zones)

```
/datalake/
  raw/    (ingestion brute par date)/topic=orders/...
  silver/ (nettoyÃ©, colonnes atomiques, types)
  gold/   (tables mÃ©tier prÃªtes BI : ventes_jour, top_SKU, forecast)
```

## 3) Traitement (Spark)

### a) Job streaming â€“ normaliser `orders` (raw â†’ silver)

```python
from pyspark.sql.functions import from_json, col, explode

orders_raw = (spark.readStream.format("kafka")
  .option("kafka.bootstrap.servers","kafka:9092")
  .option("subscribe","orders")
  .load())

schema = """
 order_id string, ts timestamp, customer_id string,
 items array<struct<sku:string,qty:int,price:double>>,
 channel string, store_id string, city string, country string
"""

json = orders_raw.selectExpr("CAST(value AS STRING) as v")\
                 .select(from_json(col("v"), schema).alias("j")).select("j.*")

# Ã©clater les items
orders_flat = json.withColumn("item", explode(col("items")))\
                  .select("order_id","ts","customer_id","channel","city","country",
                          col("item.sku").alias("sku"),
                          col("item.qty").alias("qty"),
                          col("item.price").alias("price"))

# Ã©crire en Delta/Parquet (silver)
(orders_flat.writeStream
 .format("delta")
 .option("checkpointLocation","/datalake/_chk/orders_silver")
 .option("path","/datalake/silver/orders")
 .outputMode("append").start())
```

### b) Job batch/stream â€“ agrÃ©gations (silver â†’ gold)

```sql
-- Vue ventes par jour/ville/sku
CREATE OR REPLACE TEMP VIEW v_orders AS
SELECT date(ts) AS d, city, sku, SUM(qty) AS units, SUM(qty*price) AS revenue
FROM delta.`/datalake/silver/orders`
GROUP BY date(ts), city, sku;

-- Table gold
CREATE TABLE delta.`/datalake/gold/daily_sales`
AS SELECT * FROM v_orders;
```

**IdÃ©es â€œgoldâ€ supplÃ©mentaires**

* `top_sku_7d`, `conversion_by_channel`, `stock_cover_days` (jointure stock).

## 4) Visualisation (Tableau / Power BI)

* **Connexion**: dossier `/datalake/gold/` (Parquet/Delta via connec. Spark/Databricks/ODBC).
* **Dashboards**:

  * *Sales Overview*: revenu & unitÃ©s par jour, ville, canal, top produits.
  * *Operations*: couverture de stock (jours), ruptures prÃ©dites (si forecast).
  * *Social*: courbe sentiment, top sujets nÃ©gatifs.

## 5) ObservabilitÃ© & gouvernance (indispensable)

* **QualitÃ©**: tests Great Expectations (valeurs non nulles, domaines).
* **Lineage**: Atlas/OpenLineage (rawâ†’silverâ†’gold).
* **SÃ©cu**: chiffrement au repos, **ABAC** (masquage email/PII), audit accÃ¨s.
* **CoÃ»ts**: tags FinOps, auto-stop streams inactifs.

---

# ğŸ“š Annexes pratiques

## A) Checklists de run (pour la dÃ©mo)

1. CrÃ©er topics Kafka + Schema Registry.
2. Lancer producteurs de test (`kafka-producer-perf`, scripts Python).
3. D dÃ©marrer jobs Spark (stream + batch).
4. VÃ©rifier Ã©critures `/datalake/silver` (Parquet/Delta).
5. Construire `gold` (SQL) et brancher BI.
6. DÃ©finir 3 KPIs et une alerte (latence stream > 2 min, fichiers orphelins, null rate > x%).

## B) Jeux de donnÃ©es de test (mini)

* **orders**: 5 villes, 20 SKU, trafic 50â€“200 events/min.
* **inventory**: stock initial + mises Ã  jour toutes 5 min.
* **social\_raw**: 10â€“50 msgs/min avec 20% nÃ©gatif (emoji & URLs).

## C) SchÃ©ma de dÃ©cision fraude (simple)

```
Score(model)  â†’  <0.5 ALLOW
              â†’  0.5â€“0.8 CHALLENGE (3DS/SCA)
              â†’  >0.8 BLOCK
+ Rules overrides (sanctions, pays bloquÃ©s) + Reason codes (SHAP)
```

## D) PiÃ¨ges Ã  Ã©viter

* Lake â€œraw onlyâ€ â†’ **data swamp** (mettez des contrats/qualitÃ©).
* POC notebooks sans **MLOps** (registry, monitoring, canary).
* Topics Kafka sans **clÃ© stable** (re-partitionnement casse lâ€™ordering).
* Tables â€œgoldâ€ non documentÃ©es (dÃ©finitions KPI ambiguÃ«s).

---

Si tu veux, je peux te **packager cette dÃ©mo** en deux variantes :

* **100% local** (Docker Compose: Kafka + Spark + MinIO + Superset)
* **Cloud** (Databricks + S3/ADLS + Power BI Service)

Tu prÃ©fÃ¨res laquelle pour un atelier live ?
